1-4-2023:
Update: Changed code to penalize the warping from left to right stereo images, and right to left is used for the 
	minimum reprojection loss. Smoothness loss is used for both monocular and stereo depth. 

TODO:
- If monocular depth, stereo depth, and uncertainty are being learned well, move onto pose graph optimization
- Ablation study with snake activations, direct transformation regression, rotation regularization, SGD, MSE loss, More data,
  Gauss-Newton Loss for Oxford Robotcar Dataset (See Visual Localization paper from their lab).
- Use results for research

1-8-2023:
Proposition: To address the problem with not learning stereo depth, you could do what monodepth does and use the
	     stereo image as the target image. So you can set up your code to use the monocular depth or stereo depth
	     based on the left or right target image. And then the uncertainty map doesn't need to be projected, nor does the depth
	     need to be projected. So the data will be a mix of left and right stereo images (as annotated on your text files),
	     and you only have to specify which depth map to use, and the direction of the baseline transformation. 
	     Just have to change the code to track left or right camera throughout. You could do so by creating a dictionary out of each data sample with 
	     'left' or 'right' as a key or value. This approach will use the same amount of memory on the GPU.

	     Also check the ground truth for EuRoC MAV since pitch seems to be consistently 90 degrees or close to it.

	     Also, see if there's a way to make the uncertainty map more sharp. 

1-15-2023:
Proposition: To avoid transforming the poses, uncertainty map to the right image, you could change the dataloader to return
	     dictionaries of samples containing the t-1 t and t+1 images and ts , left or right.
	     This way you can literally just look at the key in the dictionary to see if it's the left or right view.
	     Then, always pass the left image for the depth network so that Dt and Dts are always left and right image depth.
	     And pass the left or right images and their source images to the pose network. This way you don't have to pass an arg
	     for every function. Then you can change all of the code to use quantities based on if it's the left or right target image.
	     Make all of the batchs left or right for simplicity. Toggle whether you want left or right before every getitem during training and eval. Always left for test.
	     This way half the samples will update Dt and half will update Dts. Perhaps updating both Dt and Dts in this manner will draw importance to regressing the pose.
	     And the slightly different angles might make the pose network better (e.g. turns will have different values.) 

Observation: Normalization w/ ReLU seems to be working better than in the past. And it's not regressing negative z values. 
	     Also, if you ever feel the need to do some data augmentation, you could artificially balance the data by warping images by a known pose more drastically.
	     So a sample that has near 0 rotations rotates more. Also, consider changing the uncertainty lower bound again since normalizing and ReLU work together now.
	     The last time it was tested depth and pose were likely not working which meant it was a good bound for when the uncertainty was the only thing being learned well.

1-16-2023:
Update: Changed the code to alternate between left and right stereo pairs s.t. the left and right depths would both be learned. 
	Tried to run an experiment prior to this with a lower bound of 1e-6 for the uncertainty map but no learning was done for depth or pose.

Update: After a coding error, stochastic gradient descent was introduced just to see how it performs. It looks like the loss reduction
	is more controlled, and there's a slower descent into depth smoothness, but overall there's a consistent decrease (looks like exponential decay).
	Whether it will arrive at a decent solution is unclear.

Observation: Part of the issue with the uncertainty map might be the fact that the pose and depth aren't being learned well. If the residuals don't
	     get small enough, there's nothing that will prevent the uncertainty map from looking blurry. The residuals should be near 0 for most of the 
	     image to get a sharp uncertainty map.

Observation: The curves appear to be smoothly taking the loss down to a reasonable number (-0.5) but there's no progress being made on the quality
	     of the depth, uncertainty, or pose. This might suggest that even the last runs had not truly learned much information despite the loss being 
	     significantly decreased in a shorter amount of time. 

1-18-2023:
Observation: After looking at the raw devkit readme it looks like the following sequences are present in the raw data:

00: 2011_10_03_drive_0027 000000 004540
01: 2011_10_03_drive_0042 000000 001100
02: 2011_10_03_drive_0034 000000 004660
03: 2011_09_26_drive_0067 000000 000800
04: 2011_09_30_drive_0016 000000 000270
05: 2011_09_30_drive_0018 000000 002760
06: 2011_09_30_drive_0020 000000 001100
07: 2011_09_30_drive_0027 000000 001100
08: 2011_09_30_drive_0028 001100 005170
09: 2011_09_30_drive_0033 000000 001590
10: 2011_09_30_drive_0034 000000 001200

Which means the only raw (and untouched) sequences are:

2011_10_03_drive_0047
2011_09_026 (ALL) (03 seems to have been deleted anyway from the raw data)
2011_09_028 (ALL) 
2011_09_029 (ALL)

Confirm these are the sequences that you should be using ! (Probably aren't)

1. It looks like the train file you used didn't have 00 in the train files.

In your training files:
01: 2011_10_03_drive_0042 000000 001100 1.48 GB
02: 2011_10_03_drive_0034 000000 004660 7.26 GB
06: 2011_09_30_drive_0020 000000 001100 1.61 GB
08: 2011_09_30_drive_0028 001100 005170 6.09 GB
09: 2011_09_30_drive_0033 000000 001590 2.35 GB
10: 2011_09_30_drive_0034 000000 001200 1.78 GB
2011_09_26_drive_0001 114 0.177 GB
2011_09_26_drive_0005 160 0.247 GB
2011_09_26_drive_0011 238 0.370 GB
2011_09_26_drive_0014 320 0.470 GB
2011_09_26_drive_0015 303 0.450 GB
2011_09_26_drive_0017 120 0.181 GB
2011_09_26_drive_0018 276 0.427 GB
2011_09_26_drive_0019 487 0.821 GB
2011_09_26_drive_0022 806 1.30 GB
2011_09_26_drive_0028 435 0.762 GB
2011_09_26_drive_0032 396 0.508 GB
2011_09_26_drive_0035 137 0.190 GB
2011_09_26_drive_0039 401 0.557 GB
2011_09_26_drive_0051 444 0.637 GB
2011_09_26_drive_0057 367 0.466 GB
2011_09_26_drive_0061 709 1.16 GB
2011_09_26_drive_0070 426 0.655 GB
2011_09_26_drive_0079 107 0.170 GB
2011_09_26_drive_0087 735 1.17 GB
2011_09_26_drive_0091 346 0.538 GB
2011_09_26_drive_0095 274 0.418 GB
2011_09_26_drive_0104 318 0.523 GB
2011_09_26_drive_0113 93 0.151 GB
2011_09_28_drive_0001 111 0.154 GB
2011_09_29_drive_0004 345 0.546 GB
2011_09_29_drive_0026 164 0.220 GB

13726 + 8632
20.57 GB + 13.268 GB = 33.838 GB

Not in your training files:
00: 2011_10_03_drive_0027 000000 004540 6.63 GB
03: 2011_09_26_drive_0067 000000 000800 1.27 GB
04: 2011_09_30_drive_0016 000000 000270 0.397 GB
05: 2011_09_30_drive_0018 000000 002760 4.03 GB
07: 2011_09_30_drive_0027 000000 001100 1.58 GB
2011_10_03_drive_0047 1.06 GB
2011_09_26_drive_0002 0.121 GB
2011_09_26_drive_0009 0.690 GB
2011_09_26_drive_0013 0.235 GB
2011_09_26_drive_0020 0.143 GB
2011_09_26_drive_0023 0.800 GB
2011_09_26_drive_0027 0.348 GB
2011_09_26_drive_0029 0.616 GB
2011_09_26_drive_0036 1.21 GB
2011_09_26_drive_0046 0.173 GB
2011_09_26_drive_0048 0.0299 GB
2011_09_26_drive_0052 0.103 GB
2011_09_26_drive_0056 0.457 GB
2011_09_26_drive_0059 0.584 GB
2011_09_26_drive_0060 0.117 GB
2011_09_26_drive_0064 0.893 GB
2011_09_26_drive_0084 0.605 GB
2011_09_26_drive_0086 1.14 GB
2011_09_26_drive_0093 0.647 GB
2011_09_26_drive_0096 0.788 GB
2011_09_26_drive_0101 1.31 GB
2011_09_26_drive_0106 0.373 GB
2011_09_26_drive_0117 1.05 GB
2011_09_28_drive_0002 0.537 GB
2011_09_29_drive_0071 1.58 GB
2011_10_03_drive_0047 1.06 GB

20749 images
13.907 GB + 16.6699 GB = 30.5769 GB for left and right

Perhaps the list you got was optimized for depth and not odometry, hence the success on depth (in training) and not really pose.
It looks like your split is actually opposite the split that they used in the paper! They used the files you have for evaluation (at least the eval sequences)
while you don't have the other sequences in your split. This might explain the performance differences during training.
Trusting the split at first made sense because the file numbers were the same, but apparently something was done differently.

Make sure to do the mean and standard deviation calculations again for the normalization.

The splits available on Monodepth2's github are misleading or inconsistent with the D3VO paper b/c D3VO states
that 00, 03, 04, 05, 07 are in the raw data splits but the closest file is the train file under splits/benchmark (missing 03,04)

This might be because monodepth2 cites a different paper by Eigen, so the split could be different for that paper!
D3VO cites DVSO which cites "Depth map prediction from a single image using a multi-scale deep network".

It looks like you're using the eigen_zhou split from Monodepth2. 

1-19-23:
Note: The SSIM loss in wandb should be decreasing over time because it's actually (1-SSIM)/2 so 0.5 is the worst case, and this usually tends to go down once changing datasets.

Observation: The first monocular depth estimation experiment as discussed in the paper is modeled after "Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras"
	     and is not trained on the same data. Also, it appears that by Dp only this might've been with Monodepth2 and slight changes, so it has a pose network, just not D3VO's pose network

Observation: The Monodepth2 paper states that its depth network was trained on the kitti eigen split, but in the supplementary material it shows that odometry was trained on sequences 00-08.
	     So it looks like there were two models (at least) optimized for depth and pose. This explains the odom split folder (and why it has 0-8 labels).

Update: The dataset was changed. The old filenames were saved in test/train_split_old.txt in the split folder. There are less samples ~4k that expected, but this aligns with what was said in the D3VO paper in terms of preparation.
	The hope is that the data is better. If not you can also balance the data with stronger rotations by getting ground truth depth as discussed in the paper, and randomly selecting stronger poses.

1/20/23:
Update: Changed uncer lower bound to 1e-1 to see if there will be better performance. Should the uncertainty be reweighted the same way as the depth?
	Perhaps that would force the model to learn the 'variance' of the depth and it wouldn't be between [0,1], more like [0,100].
	Then the unweighted uncertainty could be used for the backend optimization. There's nothing that says you should reweight it in the paper,
	but it doesn't for depth either. 

For the next experiment reweight the uncertainty map the same way as depth. This way large residuals will be downweighted heavily and not contribute to the loss.
And very small uncertainties (easier to optimize) will be still relatively the same. In other words, not optimizing parts of the image that are dynamic or inconsistent.
But wait for this current run, because the pitch seems to be higher every once in awhile.

Update: Changed uncertainty map to reflect this observation. See 1-20-23 folder if you need to return to the last experiment. Just change back uncertainty map.

Observation: The EuRoC MAV dataset has 3x less data than KITTI so you should probably extend it to 60 epochs for EuRoC MAV (80 total)

After looking at a paper on SE(3) Optimization, supposedly, it's easier to learn rotations first and optimize for translation.
This means a network should optimize for rotations first. Maybe instead of learning like the authors suggested, you learn
from EuRoC MAV first to get the correct rotations (lower translations) and then you can use KITTI (high translations, low rotations).

Update: Starting at model d3vo_model_1_20_2023_v22, the uncertainty lower bound was changed to 1e-2 since the images were now starting to hit the lower bound.
	The epochs have also been changed to end at 80. If the uncertainty map is starting to look poor then go back to 1e-1. And make sure model doesn't overfit to euroc mav.

1/28/23:
Update: The models for testing didn't work like the training and validation scenarios because the test data was not normalized.
	Upon doing so, the output resembled the training numbers. However, it appears that the performance during the training was not good to begin with
	on KITTI. And trained on EUROC MAV, the model just over fit to the data. 

	Since the validation set was in the training distribution, you were getting false hope that the model
	was training properly. Pick a particular sequence in the training-validation split and stick with that for validation.
	This way, you'll know how well it works on out of distribution data the whole time and can stop early!

TODO:
- incoropate the liegroups library for SE(3) similar to deep pose corrections for ground truth and image warping (signs might be messing up).
- improve datasets for better generalization
- try L2 regularization (try to make magnitude < 2 so model can still approach negative infinity and likely close to 1e-3 since that's the magnitude of smoothing. So use like 1e-9 to start.)
- Also, the intrinsics for KITTI are for the original scale images, so that's another issue! (no room for making them original size though)

Update: 
- Changed image sizes so that the correct intrinsics are used. (This might also help stereo situation since in theory now pose, and stereo are known)
- Added liegroups SO3,SE3 to utility functions. After testing, the values for ground truth don't look right so it might not be a reliable library to use.
  It's also 2-3x slow because some parts have to come off the gpu to work. (Even with no uncer, stereo depth)

1-30-23:
- Noticed in the D3VO paper that the model has good generalization on Cityscapes while only being trained on KITTI. After rereading the training suggestions,
  nothing suggests a need to train on KITTI AND EuRoC MAV. Which explains why the model had poor testing performance. 
- This means that the model should reduce learning rate at epoch 15! This never was the case so it remains to be seen if this will give decent odometry results.
  (No longer training on EuRoC MAV). This also makes sense because adding EuRoC MAV to the end is like finetuning the model on EuRoC MAV specifically. 
- If this gives decent results, try improving rotation accuracy by giving the model augmentations, adjusting layers (snake activation), outputs (transformation matrix directly), etc.
- Also, trying scaled intrinsic matrix to address the fact that you're not using the images at the same size. 

Observation:
- it appears that the model is learning roll, pitch, yaw closer to the accurate pose (more so than before), so it's possible the poor intrinsic handling made it difficult to learn pose.
  (and this is all within one epoch, so it could get better too.)
- Also, the residuals for the most part look a lot better. More concentrated around cars. 


2-4-23:
- Trying to update training split to better reflect test split.
- Also need to proofread code because the model is failing to learn now. Likely a pose related issue.

Data in "Eigen Split" but not used yet:

2011_09_26_drive_0001 114 0.177 GB - train - limited moving objects, bus in distance, 2 pedestrians to the right
2011_09_26_drive_0005 160 0.247 GB - train - tight turns (rotation for pose), moving objects (uncertainty), clutter (depth)
2011_09_26_drive_0011 238 0.370 GB - val - many shadows from trees (affine params), and many dynamic obstacles (cars, bus at intersection)
2011_09_26_drive_0014 320 0.470 GB - train - shadows, overexposure (affine params), many cars (uncertainty), strong turns (pose)
2011_09_26_drive_0015 303 0.450 GB - val - 1 car leading in the beginning, more cars at the end passing (uncertainty)
2011_09_26_drive_0017 120 0.181 GB - val - Some moving cars but mostly static (uncertainty)
2011_09_26_drive_0018 276 0.427 GB - val - many passing cars while static (uncertainty), minimal moving cars while starting to drive with a turn (rotation). Since the mean optical flow might pass some sequences, and make it harder to learn.
2011_09_26_drive_0019 487 0.821 GB - train - some cars (uncertainty), over exposure and shadows from trees (affine params), slight turns (rotation). Pitchs upward b/c of speed bump. Definitely should use for training, but at the end the car is static but there are moving cars.
2011_09_26_drive_0022 806 1.30 GB - train - Many shadows from trees (affine params), many parked cars (depth), turns (pose)
2011_09_26_drive_0028 435 0.762 GB - train - very similar to the top validation sequence. Some cars, pretty much straight.
2011_09_26_drive_0032 396 0.508 GB - val - Highway scenario. Many cars (uncertainty), overexposure on wet road, and sky is completely over exposed (affine params).
2011_09_26_drive_0035 137 0.190 GB - train - Many parked cars (depth), strong turn (rotation)
2011_09_26_drive_0039 401 0.557 GB - val - Uphill. Many parked cars (depth). Pretty much straight roadway.
2011_09_26_drive_0051 444 0.637 GB - val - Many traffic related objects, parked cars (depth). Many moving cars and reflections on cars (uncertainty). Yields for a long time.
2011_09_26_drive_0057 367 0.466 GB - val - Traffic Intersection. Many moving cars (uncertainty). No motion from the car for a lot of frames. Straight away when there is motion.
2011_09_26_drive_0061 709 1.16 GB - train - Urban setting. Parked cars and buildings (depth). Slight turns
2011_09_26_drive_0070 426 0.655 GB - train - Suburban setting. Strong turn (pose). Moving car across the face of the ego vehicle (uncertainty)
2011_09_26_drive_0079 107 0.170 GB - val - Pulling over. Pretty much a straight away aside from the turn.
2011_09_26_drive_0087 735 1.17 GB - train - Tight corridor. Lots of vegetation overhead and to the sides (depth).
2011_09_26_drive_0091 346 0.538 GB - train - Down town. Many pedestrians (uncertainty), complex structures like scaffolding (depth).
2011_09_26_drive_0095 274 0.418 GB - val - Many parked cars (depth). Bright but little over exposure. Little rotation.
2011_09_26_drive_0104 318 0.523 GB - train - Urban setting. Many parked cars, close together (depth).
2011_09_26_drive_0113 93 0.151 GB - train - Some turning (pose). Pedestrian (uncertainty).
2011_09_28_drive_0001 111 0.154 GB - train - Long turn (pose).
2011_09_29_drive_0004 345 0.546 GB - train - Natural setting. Traffic (depth) and varying speeds (pose).
2011_09_29_drive_0026 164 0.220 GB - val - Many static frames, moving cars (uncertainty).

Keep in mind that loss will depend on good uncertainty estimation since matching resolution will go to -inf.

Strategy: For training use sequences with strong rotations, and leave validation for affine params, and uncer?

Validation Target: 4k samples

Training: Many rotations to help with pose. Many parked cars to help get better depth. 
	  Some over exposed scenes/ shadows; moving cars for uncertainty. 
	  Motivation: Want to learn rotations well since that's the goal, but want some hard samples to learn depth, uncertainty, affine params
Validation: Mostly over exposed scenes and shadows. More straight ways and static scenes.
	    Motivation: Want to adequately test depth, affine params and uncertainty. 
			For pose, you don't want to take all of the strong rotations. 

Sequences in training already but not explicitly stated in paper:

2011_10_03_drive_0047 1.06 GB - train - High traffic (depth). Slow, but constant moving ego vehicle (pose).
2011_09_26_drive_0002 0.121 GB - val - Urban setting. Some pedestrians. Cars are further out.
2011_09_26_drive_0009 0.690 GB - train - A good mix of moving objects, turns, brightness changes, etc. Some static frames
2011_09_26_drive_0013 0.235 GB - train - Lane changes, brightness changes.
2011_09_26_drive_0020 0.143 GB - val - Narrow lane, slow moving.
2011_09_26_drive_0023 0.800 GB - train - Many parked cars. Slight turns.
2011_09_26_drive_0027 0.348 GB - train - Straight way. Parked cars.
2011_09_26_drive_0029 0.616 GB - train - Straight way. Few cars.
2011_09_26_drive_0036 1.21 GB - train - Probably the representative validation set from the previous split.
2011_09_26_drive_0046 0.173 GB - train - Strong turn.
2011_09_26_drive_0048 0.0299 GB - val - Limited motion. Strong presence from truck (uncertainty, depth)
2011_09_26_drive_0052 0.103 GB - val - Minimal motion, many cars.
2011_09_26_drive_0056 0.457 GB - train - Long, slight turn. Some moving vehicles/train.
2011_09_26_drive_0059 0.584 GB - train - Urban/Suburban Setting. Some lane changes.
2011_09_26_drive_0060 0.117 GB - val - Intersection. Stopped ego vehicle, moving pedestrian. 
2011_09_26_drive_0064 0.893 GB - train - Urban/Suburban Setting. Turns (pose). Parked cars (depth).
2011_09_26_drive_0084 0.605 GB - train - Down Town. Ego vehicle stopped before turning.
2011_09_26_drive_0086 1.14 GB - train - Urban Setting. Sufficient Motion.
2011_09_26_drive_0093 0.647 GB - train - Urban Setting/ Side streets. Turns (pose).
2011_09_26_drive_0096 0.788 GB - train - Suburban Setting. Shade (affine params). Turns (pose).
2011_09_26_drive_0101 1.31 GB - train - Suburban Setting. Train (uncertainty). Straight way. Turn at the end (pose).
2011_09_26_drive_0106 0.373 GB - train - Urban. Many pedestraians. Different colors.
2011_09_26_drive_0117 1.05 GB - train - Urban Setting. Multiple turns (pose). Many different structures (depth).
2011_09_28_drive_0002 0.537 GB - val - Urban Setting. Many pedestrians. Long stop at intersection.
2011_09_29_drive_0071 1.58 GB - val - Urban Setting. Slow moving. Many pedestrians.
2011_10_03_drive_0047 1.06 GB - train - Highway Traffic Scenario. Slight Turn (pose).

Strategy: Trying to pick good samples from this set of data since we don't know which were used by the authors. 
	  The goal is to pick out all of the "bad" samples. Will focus on strong turns, good depth scenarios.

In Training Sequence and 00-10:

00: 2011_10_03_drive_0027 000000 004540 6.63 GB - train - Suburban Setting. Multiple turns (pose).
03: 2011_09_26_drive_0067 000000 000800 1.27 GB - null - Not in raw data
04: 2011_09_30_drive_0016 000000 000270 0.397 GB - train - Pretty much a straight way.
05: 2011_09_30_drive_0018 000000 002760 4.03 GB - train - Suburban Setting. Has turns.
07: 2011_09_30_drive_0027 000000 001100 1.58 GB - train - Urban Setting. Has turns.



Test Data:

01: 2011_10_03_drive_0042 000000 001100 1.48 GB - Large, slight turn, Highway Setting. (frequently challenging while training)
02: 2011_10_03_drive_0034 000000 004660 7.26 GB - Long sequence, Highway and Suburban Setting.
06: 2011_09_30_drive_0020 000000 001100 1.61 GB - Urban Setting, Construction.
08: 2011_09_30_drive_0028 001100 005170 6.09 GB - Suburban Setting, tight corridors.
09: 2011_09_30_drive_0033 000000 001590 2.35 GB - Long slight turn, uphill. Scenario was challenging during training.
10: 2011_09_30_drive_0034 000000 001200 1.78 GB- Long slight turn, uphill. Scenario was challenging during training.

2-6-23:
- Next Steps: Take away if-else statements to see if they're breaking the computation graph, revert back to base settings.
- Also redo the training and validation split by looking at the distribution of poses. Try to get a balance of strong rotations, and eliminate all no motion sequences.
