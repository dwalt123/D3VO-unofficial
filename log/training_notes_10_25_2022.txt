10/10/22

Changed SSIM from torchmetrics to SSIM from kornia. Changed l1 loss sum
reduction to mean reduction to see if the learning of depth and pose 
was more balanced (not heavily toward depth or pose).

10/12/22

When reintroducing the affine transformations with the kornia warping function,
the affine transformations were applied to the image warping function instead of
the target image. This gives a noticeable increase of initial loss 
from 10k-15k -> 20k-25k, suggesting more room for improvement now. In the 
other case, you were essentially learning to keep the affine transformation
to get better results. Now the affine transformations are learned to correct
the target image.

Observations and Questions:
The pose is slowly becoming larger than 0.03 (e.g., 0.04-0.06), but could the 
ReLU be making it harder to regress larger values? Should you go back to LeakyReLU
now that the affine transformations were put in the right place? Also, since EuRoC MAV
has larger transitions and rotations, could that help with learning pose, and KITTI is just
a good way to learn depth first? 

Update: computer began to loudly shake, so the experiment was cut for a few days (10/12-10/17)

10/17/2022:
Resuming experiment after computer shaking incident. 
Still a massive artefact at the right edge of the images for some reason

10/18/22:
The code in the past uses torch.mean to calculate the normalized residual. 
This means the uncertainty map is casting to a batch of residuals. This is likely only
normalizing the mean of the image. So you should apply the torch.mean to the uncertainty map
and torch.log(torch.prod(uncertainty_map)). Otherwise you have to make SSIM and L1 Loss a pixel wise operator
(which might actually be better based on the paper.) If you use 'none' for reduction you can do this!

Update: Kornia's multiscale loss is too slow (3+ seconds per training step) so L1 loss and their SSIMLoss reduction were changed to none in the original
	to see if there are any significant changes. Training loss started at 0.747

Update: Trying without normalizing the residual functions, to see if depth performs better this way. Images continue to be blank when using mean reduction. 
        Initial training loss: 230534

10/19/22:
The training loss is more so in the 150k range, and the depth maps look visually better than the previous trials.

Important Consideration:
The affine parameters are being learned well. a is closer to 1, and b is closer to 0. However, the pose is regressing small numbers (e.g., 0.02). 
Perhaps, the posenet model is having trouble learning the (likely bigger) weights of the pose. The pose block is a single linear mapping to the output dimension.
Since a sum or none reduction loss benefits from mostly reducing error in depth, and the affine parameters for KITTI are in the range [0,1]. It would make sense
that the model has yet to learn the proper weights for pose (not obvious that pose has a large impact of the error.) So initially, the network is learning to 
encode information that benefits the affine parameters mostly. The challenge is learning how to take that encoding, and map it to pose in one layer.
Also consider, that the loss might begin to drop off once the dataset has changed since the depth has been learned, but the poses for EuRoC MAV will change more drastically.
So the strategy is to learn depth sufficiently well with KITTI, and Pose sufficiently well with EuRoC MAV. 

10/20/22:
The model is now working for EuRoC MAV. May have to check the stereo baseline calculations, but it's saving images.
It looks like the training loss for EuRoC MAV started as high as 220k-300k, and came down to as low as 30k-70k. Will have to see improvement over time!

After the first epoch on EuRoC MAV, the avg training loss was 65k! Second was 43k. Training time takes ~30 min/epoch now. 
Validation Loss: 37k for second epoch on EuRoC MAV, Validation time: ~3 min

The ground truth appears to be 0 for most batches. Check stereo baseline calculations. An inverse or other calculation might be causing this.

Accidentally overwrote the output for the 20th KITTI run (epoch labeled 19). For visualization this won't matter but this means the last folder will be 58 (-1 because 0 indexed, -1 b/c of overwrite mistake)

The depth looks good for EuRoC MAV. There are significant improvements. However, the pose regression is still struggling. Perhaps you should focus on pose with KITTI
to get a smoother baseline. And then focus on learning depth with EuRoC MAV? It seems like it does particularly well early on. If this is the case,
go back to mean reduction for the residual loss, and hope the depth begins to change after the 20th epoch. 

You could also let it run through the 60 epochs now, and then change to mean reduction to see if it'll start learning pose, now that you've learned depth well. 

10/21/22 Update:
Model finished running (it only took 3 Days!)
The depth although it worked for some cases, it failed to look perceptually well for others. 
This is possibly because the pose and depth were not jointly being learned well. So even though the network learned
to predict depth well for the training set, it didn't generalize well, creating some visually erroneous images.

Solution: Fix the network early on until you're confident both depth and pose are being adequately learned at the same time.

Also check out MonoRec. The same lab that designed D3VO came out with a new paper called MonoRec that has 
an open source implementation. It might give some insights on their software engineering practices that could help
debug some issues: https://github.com/Brummi/MonoRec . It looks like they also used kornia for this project and 
they have their own SSIM class. And a lot of these classes have forward functions! This could probably be the issue, 
since you're not using a backprojecting class or ssim class, the warping function itself is not learning directly how to warp
images. Since MonoRec also uses Pytorch, read the paper and see if they have the warping process directly implemented in their code!

Also, D3VO says "The perpixel minimum loss is proposed in Monodepth2 " refering to the min() function in the residual calculation,
so you have to do the min() for every residual element, not the whole thing! Also, it looks like SSIM is calculated for every pixel as well,
not the whole image. This might make a difference because instead of having varying scores throughout the image, it's constant.

These observations might explain why the model is regressing constant images.

If you look at kornia's source code (https://github.com/kornia/kornia/blob/master/kornia/losses/ssim.py), the functions do in deed have 
a forward function for each class, which should make them differentiable in pytorch. So if you use perpixel SSIM and L1 Loss, you're
differentiating each pixel by their own unique value, rather than the SSIM and L1 Loss of the entire image! 

Also looks like MonoRec and Monodepth use the same differentiable SSIM function! 

*** Although the kornia.geometry.depth.warp_frame_depth warps an image using depth, it doesn't have a forward pass!
    Therefore pytorch has to guess! It's not technically differentiable in this sense.***
Solution: learn to use DepthWarper() class in kornia since it has a forward pass. Pay particular attention to
	  how the pose and depth would be differentiated in the forward pass.

Action Items:
- Use mean reduction for SSIM and L1 loss and use DepthWarper so that you have a warping function with a forward pass!
  Need to investigate PinholeCamera and how they compute the projection matrix
1. Change residual to min(residuals) then normalize with uncertainty map
- Explore MonoRec, Monodepth2, Kornia, to see how they use their forward functions and see how they calculate 
residuals per pixel. This should help!
- Write these details on paper to see the connections

Sidenote: kornia also has nerf code (see github) https://github.com/kornia/kornia/tree/master/kornia/nerf

Update:
- Wrapped kornia image warping function into own personal image warping function (no pinhole camera model needed)
  so now the warping function should be differentiable wrt to the pose and depth
- also changed the loss function to get the minimum of all elements in the residual to make the loss per pixel like the paper.

Consider passing all loss functions into the loss forward and require gradients (and check them) to see if this will force gradients to depend on 
pose, depth and/or using monodepth2 backprojection class since it has a forward pass. This makes it directly differentiable!

Changing SSIM Loss, Backprojection and Projection mechanics so that there's a forward function explicitly for pytorch to make gradients

Update: Added a class for every major function in TotalLoss. Hopefully this will make all of the underlying parameters differentiable, and improve the model.
	Didn't add a class for SSIM, still using kornia. However, if there's room for improvement still you can use Monodepth2's

Update: Changed the initial learning rate to 5e-4 instead of 1e-4. The pose immediately changed to in the 20 m range which is now too high. Work backwards
	to force the scale to be in the meter range. And then hopefully the model will learn the appropriate depth that goes along with the pose. 

Update: Changed it back to the original learning rate. Only major change is the location of the pose_optimizer.step() and depth_optimizer.step() methods.

10/23/22 Update:
Decided to chain the model.parameters() together with itertools.chain() like CycleGAN supposedly does to see if you can optimize the parameters better with
one optimizer (potentially avoiding only pose or depth being learned?) 

Update: Added scale exponent to smoothness and regularization term (just like lambda). Unsure if this is correct. Currently the loss is ~50M which is really big.
	but the pose and affine parameters are learning much faster. Might need to somehow swap the exponents so that the smallest scale is ^4 and the full sized scale is ^1.

10/24/22: The pose estimates are much larger than before and the depth is making slight improvements (for now). The loss went down a lot (50M -> <1k) in just 3 epochs
	  For the next iteration change int(key) + 1 -> 4-int(key) so that the larget scale is raised to the power 1 and smaller scales to the power 4. This will emphasize
	  making the smallest scale more like the encoding counterpart, and then that should propagate up in the scale.

	  Also note the current z values are regressing negative values which is also problematic. 

	  At this point in training the loss is <100. So the training in 3 epochs led to a decrease in magnitude of almost 6 (50M -> <100) and eventually got as low as <10 consistently.
	  The goal should be to hit 0.1 per batch.

Update: The loss was changed to give smaller scales more weight. The loss started at 1.55M and quickly went down to a magnitude closer to 10k.

10/25/22:

The resnet implementation in pytorch reassigned each layer to the input value. Maybe this helps the autodifferentiation of pytorch?
Had to move around many layers in depth net to mimic this behavior. Reference paper if there's any weird behavior.

Noticeable change: almost immediately the pose was > 0.001. And affine transformations quickly got to the expected values (e.g., 0.050 pose, 1 a, 0 b)

10/26/22:
After rereading a portion of the D3VO paper, the DepthNet might've been taken directly from monodepth2. This might resolve some concerns with the depth network.
Use the code from the depthnet layers in monodepth2 and see if there are any changes.

Also noticed the depth network does use the approach where the input is modified and passed until the output

Side Note: Plot the weights of the model after it's done training (or do so for every epoch) so that you have
	   a heat map of every layer to see how high or low each activation is being changed throughout training.
	   There might be some interesting observations from looking at the training, like activations for the model at each layer over time.
	   Could do this in 3D or just save a heat map at every layer. Also, you could show it at inference time to see how activations react
	   in some scenarios vs others.

Comparing Monodepth2 and D3VO's DepthNet Decoder, it's the same model, but they change the disp1-4 to disp_uncer1-4 by changing the output channels from 1 to 3.
Also, Monodepth2 mentions that they change the scale of the images to be the same for the loss function. 
Also, in the supplementary material it says they mask out exposure pixels because it impacts how the affine parameters are learned!
Monodepth2 also has classes for decoding the pose. So you could make small classes for pose, a, b

Action items:
1. Get monodepth2 depthnet to work or adapted to your code (might be a glaringly obvious change needed that fixes things)
2. Change posenet so that pose,a,b have their own classes (might just be updating encoder which would explain the behavior of the net: only a/b or pose are accurate!)

Update:
Didn't change action item too, because the pose_encoder was implemented with functions too, so if that assumption was
true, the pose_encoder wouldn't be updated either. 

Was able to implement the Monodepth2 depthnet. Will have to monitor the progress and see if the affine parameters still give bad results.
If so, go back to the papers.

Next Update: 
- Pytorch says it makes the most sense to do requires_grad at the leaf nodes (input) and no where else. Make sure there are basic operations
  from the output to the input and that they're differentiable.
- Also explore how to mask regions of exposure. (LSD SLAM with stereo cams clamps the residual value)
- Consider warping the image to the next image and visually how well it does. It might give insight on why things
are behaving oddly.
- use disparity instead of depth for smoothness function. 

Update: Haven't attempted the exposure masking yet. Need to figure out what threshold to use. Updated the requires_grad_ to be for input
        Using pseudo disparity for smoothness. The stereo depth might be better at first because the baseline transformation is consistent 
	and likely a better option than the regressed pose. Consequently, the depth looks better since the pose is known. To resolve this,
	the model will likely have to learn for them to look about the same. 

Another detail: The cited paper in d3vo (left-right consistency) uses disparity and it uses the mean for the smoothness loss. D3VO uses sum.
                This might be why the model allows itself to regress low a, and high b values. Loss_smooth >> Loss_ab
		
		The highest Loss_ab should be 8 per batch -> 16. And beta makes it contribute at most 0.16. Maybe it should be the mean so that the affine params are near this magnitude?
		Mean is also used in Monodepth2 implementation!

		Changing the smoothness loss to mean caused the loss to go down to 0.04. So the exponential terms for scale were removed. It started at ~ 0.2

		Changing back to sum loss_smooth, because that will promote higher variance in the depth data, since the gradient is inverse depth
		and this should force the model to learn varied depth. Loss started at 2 this time.

10/27/22:
Turned smooth loss and residual loss back to sum, changed inverse depth to regular depth. Hoping to learn depth like before, but with a differentiable pose network this time.
Adding the scale exponents to see if this also adds an improvement. Because of these points, improved the depth and pose respectively.
Loss is now 18,820,196,651,237,376 (18 quadrillion) at the start.

Removed scaling exponential because the loss was not decreasing and no learning was occuring.



Started at 161,737.890625

* Maybe pose isn't being regressed properly because you're passing lists of transformation matrices, and those aren't directly differentiable without lie algebra
  Make a differentiable transformation matrix function and pass the parameters directly. 

It looks like Monodepth2 passes the outputs of the network with the transformation matrix in a dictionary.
This probably forces the network to differentiate wrt the pose, rather than passing an already processed output to the loss


11/1/22:

Consider making a video of how each layer is changing in the networks

Set Conv Layer, and Out layers to bias=True. The initial pose values, are now non zero

True bias for all layers made the affine parameters slowly go to zero for a and higher for b. 

Making the bias for the conv, and pose true, but false for a,b makes the pose snap quickly to a plausible answer
(e.g, z = 1.5) but a -> 0, b -> 1

Added a mask (see torch.where block in residual_loss) at 0.9 threshold. Changed bias for all back to False

Added bias to pose out block, but it might still be effecting the affine parameters -> weird

Update: changed the resizing so that the scales would resize and not the input images. Had to cut batch size to 4 
	to fit in memory. Also running with a bias currently. This time it's not decreasing immediately.

Update: Reintroduced scaling, no bias. Fixed the lambda coefficient. Right now it looks like the scaling forces
	the smallest scale to be better which means there's less weight on how the following blocks are scaled.
	Some early observations show that the affine parameters are at appropriate values, and the pose is nonzero but small still
	(e.g., >0.01 instead of 0.001). Some z values are positive, some are negative. This might be a good starting point to start at.

Also note that residual loss can get < 1 so when you use exponentials, that will go to 0. 
So is the model learning to smooth as much as possible? (ie flat image) 

It might be worthwhile to see the images at each scale, since the losses are weighted differently!
Save them in different folders

Changed lambda from 1e-3 -> 1e-4 because the loss for the residual was approx. 10x smaller than the reg loss,
which likely explains why the loss was going down, but the images were flat, i.e., too much regularization.
If this fixes the issue completely it's unclear why 1e-3 was reported in the paper. 

Update: Looking at the Monodepth2 paper again, they use the mean normalized inverse depth to discourage shrinking of the depth.
	They might've meant that they use it to discourage the shrinking of the gradient. So 1e-3 was reset and the inverse was taken
	and the depth was mean normalized.

11-2-22: 

Update: Changed loss_self to sum reduction and added a bias to the posenet again. The depth was still not being learned
	with the mean normalized depth. Now it looks like the pose is being learned and there's more weight associated with 
	the residual loss.

Update: Took out smoothing loss and affine parameters to see if it works at all in terms of learning to get pose
	from warping. If it does you need to fix the affine and smoothness loss. If it still doesn't learn, something
	more fundamentally wrong is going on! Also, taking out affine adjustments since they might not be learned

Update: Using custom SSIM class instead of kornia since other papers besides Monodepth2 use the exact same implementation


Update: Since the pose is still not changing that much, it might be the uncertainty map being too strong of a regularizer,
	so that was capped to 80 like the depth maps. It looks like this improved the model quite well after one epoch.
	No smoothness regularization or affine regularization.

11-3-22:
Adding the smoothness term made the images completely smooth again. It must be too strong still.
Using 1e-5 for lambda to force residual error to be 10x greater to start.

Update: The images still became smooth when reintroducing smoothness loss at 1e-5. Trying with mean normalized inverse depth again
	and more regularization to see if it's any different. Proceed with just loss_self and loss_ab if it still doesn't work.
	Investigate what might be going on.

Update: Removed loss_smooth to avoid over smoothing the depth maps. It didn't work for inverse depth or regular depth. Even with lambda of 1e-5 it's still too much smoothing.
	Changed lambda back to 1e-3. If images still smooth l_ab might have some unintended impact on smoothing, since without any reg. it performed well.
	
	If it works well, something is wrong with the smoothing in terms of scale, amount of regularization, etc.
	
Update: Try printing the pose and some of the warping results while training to debug why stereo and monocular depth aren't learning well.

11-4-22:
Noticed the artefacts might not go away because there's no smoothness constraint. 
Changed smoothness loss to mirror that in Monodepth2. Try it without mean normalized inverse depth, and mean next if it doesn't work well.
Also, look at why pose isn't being learned.

Update: Added the smoothing back using sum reduction and depth. Noticed that for the early blocks it is smoothed more initially and
	for the last block the down weighting is about the same as the residual loss. 

Update: Changed the edge aware smoothness loss to reflect the approach in Monodepth2. 

Update: Stereo depth is struggling to be learned, monocular depth is okay. The pose is not being learned well.
	The poses seem reasonable and the tensors have grad_fn's so they should be differentiable. Added bias back into posenet.

11-5-22:
Update: It appears that in SfMLearner, their pose prediction output is done with convolutional layers (!!!)
	Not a linear output for pose, a, b. This might be why there have been issues learning pose. If this doesn't work,
	try multiplying output (like SfMLearner) by 0.01. The last dim was averaged to get the output in the right shape.

Unclear why stereo depth is not being learned well. (Min res loss causing only monocular depth portion to be learned?)
Additionally, it's not clear why pose isn't being learned well. It seems to be stuck.(Mult. by 0.01 and wait?)

Update: Using Randomized ReLU (RReLU) instead of LeakyReLU or ReLU to see if the learning stops around 0.02 still
	Early on it appears that it is learning higher values sooner. However, if this ultimately doesn't work you can try 
	normalizing the data again to see how (if at all) it impacts the output. Large numbers eventually being learned (e.g., 10, 0.1-2 meters expected at 10 Hz)
	You might need to multiply by 0.01 like sfmlearner so that the model learns to settle in the range of values. It might continuously grow otherwise.

Update: The model was learning to wipe out the image. Multiplied by 0.01 like SfMLearner. 

Update: The affine parameters began to give poor results very quickly. Went back to ReLU and no forced (0.01) scaling.
	Normalizing the images now to see if that helps fix the problem. Trying to fix the vanishing gradient problem.

11-6-22:

Update: Looking at the stereo baseline more closely, it looks like the provided calibration data had a flipped convention so
	you were actually projecting the image to the right of the right camera instead of back to the left stereo image.

	This alone might be causing issues with learning the pose? Even though it has no gradient perhaps it's making it difficult.

Update: Reintroduced RReLU with stereo baseline change to see if anything positive will happen.
